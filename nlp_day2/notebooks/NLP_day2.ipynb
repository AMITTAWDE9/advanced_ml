{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\"> Natural Language Processing - 102 </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Program so far \n",
    "***\n",
    "\n",
    "* Python\n",
    "* Statistics\n",
    "* Supervised Machine Learning\n",
    "* Unsupervised Machine Learning\n",
    "* NLP 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda for the Day\n",
    "***\n",
    "\n",
    "\n",
    "2. POS Tagging\n",
    "- Chunking\n",
    "- Parsing\n",
    "- Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We saw in the previous session the bag-of-words approach. As we discussed, bag-of-words fails to capture the structure  of the sentences. Part of Speech helps us overcome this weakness. Let's see how"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "div.cell { /* Tunes the space between cells */\n",
       "margin-top:1em;\n",
       "margin-bottom:1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render h1 { /* Main titles bigger, centered */\n",
       "font-size: 2.2em;\n",
       "line-height:1.4em;\n",
       "text-align:center;\n",
       "}\n",
       "\n",
       "div.text_cell_render h2 { /*  Parts names nearer from text */\n",
       "margin-bottom: -0.4em;\n",
       "}\n",
       "\n",
       "\n",
       "div.text_cell_render { /* Customize text cells */\n",
       "font-family: 'Times New Roman';\n",
       "font-size:1.2em;\n",
       "line-height:1.4em;\n",
       "padding-left:3em;\n",
       "padding-right:3em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "\n",
    "div.cell { /* Tunes the space between cells */\n",
    "margin-top:1em;\n",
    "margin-bottom:1em;\n",
    "}\n",
    "\n",
    "div.text_cell_render h1 { /* Main titles bigger, centered */\n",
    "font-size: 2.2em;\n",
    "line-height:1.4em;\n",
    "text-align:center;\n",
    "}\n",
    "\n",
    "div.text_cell_render h2 { /*  Parts names nearer from text */\n",
    "margin-bottom: -0.4em;\n",
    "}\n",
    "\n",
    "\n",
    "div.text_cell_render { /* Customize text cells */\n",
    "font-family: 'Times New Roman';\n",
    "font-size:1.2em;\n",
    "line-height:1.4em;\n",
    "padding-left:3em;\n",
    "padding-right:3em;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**What is Part of Speech tagging?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part of Speech tagging (POS tagging):** POS tagging is defined as the process of marking words in the corpus corresponding to their suitable/particular Parts of Speech. \n",
    "\n",
    "\n",
    "* The __set of tags__ is called the __tag set.__\n",
    "\n",
    "\n",
    "* Standard tag set is : __Penn Treebank__ (This is for English language.)\n",
    "\n",
    "\n",
    "* The POS tags of the word is __dependent__ on both __its definition and its context.__\n",
    "\n",
    "\n",
    "* POS tags of the words are dependent on their relationship with adjacent and related words in the given phrase, \n",
    "  sentence and paragraph\n",
    "  \n",
    "\n",
    "* It is also called grammatical tagging or word-category disambiguation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ![Image_1_2.jpg](../Images/Image_1_1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Sentence: **John likes the blue house at the end of the street.**\n",
    "\n",
    "\n",
    "| Words  | Word Category   | \n",
    "|---|---|\n",
    "| John  | Noun |\n",
    "| likes | Verb  | \n",
    "| the  |  Determiner |\n",
    "| blue |  Adjective | \n",
    "| house  | Noun |\n",
    "| at | Preposition  | \n",
    "| the| Determiner  |\n",
    "| end | Noun  | \n",
    "| of  | Preposition  |\n",
    "| the |  Determiner | \n",
    "| street |  Noun |\n",
    "| .      |  . |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example__\n",
    "\n",
    "* She saw a __bear.__  :  Here, __bear__ is __\"Noun\"__\n",
    "\n",
    "* Your efforts will __bear__ fruit.: Here, __bear__ is __\"Verb\"__\n",
    "\n",
    "* Muje __khaana(1) khaana(2)__ hai. (Hindi Language) : Here, __khaana(1) is \"Noun\"__ and __khaana(2) is \"Verb\".__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Part of Speech tagger?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part of Speech tagger (POS tagger):** POS tagger is the tool that is used to assign POS tags for the given sentence or dataset/corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the various types of the Part of Speech tags?**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image_1_3.png](../Images/Image_1_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image_1_4.png](../images/Image_1_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\"> You can see the list of tags from <a href =\"https://github.com/jalajthanaki/NLPython/blob/master/ch5/POStagdemo/POS_tags.txt\" target=\"_blank\"> here </a></p>\n",
    "\n",
    "\n",
    "<p style=\"text-align: center;\"> You can see the tags with example from <a href =\"https://www.sketchengine.eu/penn-treebank-tagset/\" target=\"_blank\"> here </a></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which applications are using POS tagging? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Word Sense Disambiguation\n",
    "    \n",
    "\n",
    "* Grammar correction system\n",
    "\n",
    "    \n",
    "* Question-Answering system\n",
    "\n",
    "    \n",
    "* Machine Translation\n",
    "\n",
    "    \n",
    "* Sentiment Analysis\n",
    "\n",
    "\n",
    "* Detection of Multi word Expression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Part of Speech tags are grammatical consituents (Noun, Verbs, Adverb, Adjectives) and this process of POS tagging classify tokens into their part-of-speech tags and label them according the tagset which is a collection of tags used for the pos tagging. Part-of-speech tagging also known as word classes or lexical categories. Here is the definition from wikipedia:\n",
    "    \n",
    "<i>In corpus linguistics, part-of-speech tagging (POS tagging or POST), also called grammatical tagging or word-category disambiguation, is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition, as well as its context—i.e. relationship with adjacent and related words in a phrase, sentence, or paragraph. A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.\n",
    "\n",
    "Once performed by hand, POS tagging is now done in the context of computational linguistics, using algorithms which associate discrete terms, as well as hidden parts of speech, in accordance with a set of descriptive tags. POS-tagging algorithms fall into two distinctive groups: rule-based and stochastic. E. Brill’s tagger, one of the first and most widely used English POS-taggers, employs rule-based algorithms.<i>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "tokens = word_tokenize(\"And now for something completely different\")\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These DT, NNP, MD etc are pos tags taken from the standard list of Penn TreeBank Tagsets. It can be found here\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "POS tagging is one of the basic and very important component of NLP, as NLP mainly works on linguistics, i.e. way of writing language and Grammar is important part of it. POS tagging in the world of NLP is solved problem and works well if language is written well formatted.\n",
    "\n",
    "POS tagging is also supervised learning solution that uses features like previous word, next word, is first letter capitalized etc.\n",
    "\n",
    "NLTK has a function to get pos tags and it works after tokenization process. \n",
    "\n",
    "In our problem of Author Identification, we can create multiple features using POS Tagging.\n",
    "1. Number of Nouns, Verbs, Adjectives etc.\n",
    "2. How many times sentence starts with Adverb. Meaning words like Basically, Typically etc.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n"
     ]
    }
   ],
   "source": [
    "# We can get more details about any POS tag using help funciton of NLTK as follows.\n",
    "nltk.help.upenn_tagset(\"RB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Understanding Lexical Units:\n",
    "\n",
    "* Approximately 8 traditional basic word classes, sometimes called lexical classes or types\n",
    "* These are the ones taught in grade school grammar\n",
    "    - N noun chair, bandwidth, pacing\n",
    "    - V verb study, debate, munch\n",
    "    - ADJ adjective purple, tall, ridiculous (includes articles)\n",
    "    - ADV adverb unfortunately, slowly\n",
    "    - P preposition of, by, to\n",
    "    - CON conjunction and, but\n",
    "    - PRO pronoun I, me, mine\n",
    "    - INT interjection um"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Open Class\n",
    "\n",
    "* Can add words to these basic word classes:\n",
    "* Nouns, Verbs, Adjectives, Adverbs.\n",
    "    - Every known human language has nouns and verbs\n",
    "* Nouns: people, places, things\n",
    "    - Classes of nouns\n",
    "* proper vs. common\n",
    "* count vs. mass\n",
    "    - Properties of nouns: can be preceded by a determiner, etc.\n",
    "* Verbs: actions and processes\n",
    "* Adjectives: properties, qualities\n",
    "* Adverbs: hodgepodge!\n",
    "    - Unfortunately, John walked home extremely slowly yesterday\n",
    "* Numerals, ordinals: one, two, three, third, …"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Closed classes\n",
    "\n",
    "* Words are not added to these classes:\n",
    "    * determiners: a, an, the\n",
    "    * pronouns: she, he, I\n",
    "    * prepositions: on, under, over, near, by, …\n",
    "    * over the river and through the woods\n",
    "    * particles: up, down, on, off, …\n",
    "* Used with verbs and have slightly different meaning than when used as a preposition\n",
    "    - she turned the paper over\n",
    "* Closed class words are often function words which have structuring uses in grammar:\n",
    "    - of, it , and , you\n",
    "* Differ more from language to language than open class words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'DT'),\n",
       " ('dog', 'NN'),\n",
       " ('saw', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('man', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('park', 'NN')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These DT, NNP, MD etc are pos tags taken from the standard list of Penn TreeBank Tagsets. It can be found here\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "POS tagging is also supervised learning solution that uses features like previous word, next word, is first letter capitalized etc.\n",
    "\n",
    "NLTK has a function to get pos tags and it works after tokenization process. \n",
    "\n",
    "In our problem of Author Identification, we can create multiple features using POS Tagging.\n",
    "1. Number of Nouns, Verbs, Adjectives etc.\n",
    "2. How many times sentence starts with Adverb. Meaning words like Basically, Typically etc.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n"
     ]
    }
   ],
   "source": [
    "# We can get more details about any POS tag using help funciton of NLTK as follows.\n",
    "nltk.help.upenn_tagset(\"RB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Chunking is a process of extracting phrases(aka chunks) from unstructured text. Instead of just simple tokens which may not repersent actual meaning of text, its advisable to use phrases such as \"New Delhi\" as a single word instead of New and Delhi separate words.\n",
    "\n",
    "- Chunking is done using linguistic rules(language grammar rules), such as when two proper nouns occur together, merge them to make a single word. For Example \"South Africa\".\n",
    "-  Chunking works on top of POS tagging, it uses pos-tags as input and provide chunks as output. \n",
    "-  Similar to POS tags, there are standard set of Chunk tags like Noun Phrase(NP), Verb Phrase (VP) etc.\n",
    "-  Most data scientist uses N-Grams instead of chunker, but n-grams ends up creating a lots and lots of meaningless words.\n",
    "-  Chunking is very important when you want to extract information from text such as Locations, Person Names etc. In NLP called Named Entity Extraction.\n",
    "-  In Author Identification, we can hvae features like how many Named entity author uses in a sentence.\n",
    "-  What kind of countries/continents, author mostly refer in his articles.\n",
    "\n",
    "There are a lot of libraries which gives phrases out-of-box such as Spacy or TextBlob. NLTK just provides a mechanism using regular expressions to generate chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Unidentified/JJ)\n",
      "  hackers/NNS\n",
      "  (VP gained/VBD)\n",
      "  (NP access/NN)\n",
      "  to/TO\n",
      "  (NP the/DT department/NN)\n",
      "  's/POS\n",
      "  (NP web/JJ page/NN)\n",
      "  (P on/IN)\n",
      "  August/NNP\n",
      "  16/CD\n",
      "  and/CC\n",
      "  (VP replaced/VBD)\n",
      "  it/PRP\n",
      "  (PP (P with/IN) (NP a/DT hate-filled/JJ diatribe/NN))\n",
      "  (VP labelled/VBD)\n",
      "  (NP the/DT)\n",
      "  Department/NNP\n",
      "  (P of/IN)\n",
      "  Injustice/NNP\n",
      "  that/WDT\n",
      "  (VP included/VBD)\n",
      "  (NP a/DT swastika/NN)\n",
      "  and/CC\n",
      "  (NP a/DT picture/NN)\n",
      "  (P of/IN)\n",
      "  Adolf/NNP\n",
      "  Hitler/NNP\n",
      "  ./.)\n",
      "(NP Unidentified/JJ)\n",
      "(VP gained/VBD)\n",
      "(NP access/NN)\n",
      "(NP the/DT department/NN)\n",
      "(NP web/JJ page/NN)\n",
      "(P on/IN)\n",
      "(VP replaced/VBD)\n",
      "(PP (P with/IN) (NP a/DT hate-filled/JJ diatribe/NN))\n",
      "(P with/IN)\n",
      "(NP a/DT hate-filled/JJ diatribe/NN)\n",
      "(VP labelled/VBD)\n",
      "(NP the/DT)\n",
      "(P of/IN)\n",
      "(VP included/VBD)\n",
      "(NP a/DT swastika/NN)\n",
      "(NP a/DT picture/NN)\n",
      "(P of/IN)\n"
     ]
    }
   ],
   "source": [
    "#Define your grammar using regular expressions\n",
    "#For example a phrase starting with determiners(The/an/a) followed by noun or adjective will be a noun phrase. such as \"a greedy dog\"\n",
    "parser = ('''\n",
    "    NP: {<DT>? <JJ>* <NN>*} # NP\n",
    "    P: {<IN>}           # Preposition\n",
    "    PP: {<P> <NP>}      # PP -> P NP\n",
    "    VP: {<V.*> <PP|RB|V.*>*}  # VP -> V (NP|PP)*\n",
    "    ''')\n",
    "line=\"Unidentified hackers gained access to the department's web page on August 16 and replaced it with a hate-filled diatribe labelled the Department of Injustice that included a swastika and a picture of Adolf Hitler.\"\n",
    "chunkParser = nltk.RegexpParser(parser)\n",
    "negation_result={}\n",
    "tagged = nltk.pos_tag(nltk.word_tokenize(line))\n",
    "tree = chunkParser.parse(tagged)\n",
    "negated_entity=\"\"\n",
    "negated_value=\"\"\n",
    "negation=None\n",
    "for subtree in tree.subtrees():\n",
    "    print (subtree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "gensim(https://radimrehurek.com/gensim/) package in python implements most of topic modelling algorithms.\n",
    "\n",
    "* We'll walk through a basic application of Topic Modeling with LDA\n",
    "* We'll also cover the basic NLP operations necessary for the application\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# create sample documents\n",
    "# We will use author data\n",
    "import nltk\n",
    "text=open(\"../data/C50train/AaronPressman/2537newsML.txt\").read()\n",
    "sents = nltk.sent_tokenize(text)\n",
    "\n",
    "# compile documents\n",
    "doc_complete =sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One of the advance topic in NLP is Lexical Analysis of text wherein we try to analyze and understand text. This process is called deep tree parsing in NLP world where we try to analyze relationships amongst the text.\n",
    "- Text parsing is important when you want to know relationships in text. For example <i>Delhi is capital of India<i>, here Delhi and India are related and having a relationship <b>is capital of<b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (Det the) (N dog))\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (Det a) (N man) (PP (P in) (NP (Det the) (N park))))))\n",
      "(S\n",
      "  (NP (Det the) (N dog))\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (Det a) (N man))\n",
      "    (PP (P in) (NP (Det the) (N park)))))\n"
     ]
    }
   ],
   "source": [
    "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  VP -> V NP | V NP PP\n",
    "  PP -> P NP\n",
    "  V -> \"saw\" | \"ate\" | \"walked\"\n",
    "  NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n",
    "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
    "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
    "  P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
    "  \"\"\")\n",
    "sent = \"the dog saw a man in the park\"\n",
    "tokens=nltk.word_tokenize(sent)\n",
    "rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
    "for tree in rd_parser.parse(tokens):\n",
    "    print(tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../images/deep_parsing.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here as well we have to define our grammar, which looks quite tedious job. But there are other NLP packages such as Stanford CoreNLP which provides funcitons to generate parse tree from unstructured text without defining any grammar.\n",
    "- Parse tree provides us meaningful and true relations and also kind of relations they share. Also called facts.\n",
    "- Tree Parsing is used to build knowledge base from unstructured corpus. Check DbPedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "So far we have visited topics for supervised machine learning for NLP. Now let's see some upsupervised machine learning techniques for NLP.\n",
    "\n",
    "\n",
    "NLP is all about unstructured data, and one of the problem industry is facing today is about amount of data that any System has to process. Often its not practical to read through a huge volume of data and get some insights about that data. Consider google news, there are hundred of thousands of news get published on daily basis. So we need a way to group news with some keywords in order to understand what is going on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../images/topic_modelling.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One in red are classes, which are fixed and with the help of training data, we can build news classifier.\n",
    "- But one in green are topics, that are identified run time. And process of identification of topics is totally unsupervised. And Topic modelling is one the best way to understand, repersent any unstructured text without actually getting into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Topic Modelling__ as the name suggests, it is a process to automatically identify topics present in a text object and to derive hidden patterns exhibited by a text corpus. Thus, assisting better decision making.\n",
    "\n",
    "A __Topic__ can be defined as “a repeating pattern of co-occurring terms in a corpus”. A good topic model should result in – “health”, “doctor”, “patient”, “hospital” for a topic – Healthcare, and “farm”, “crops”, “wheat” for a topic – “Farming”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Document Clustering.\n",
    "    1. Group news.\n",
    "    2. Group emails.\n",
    "    3. Group similar medical notes etc.\n",
    "- Keywords Generation. Can be used for SEO.\n",
    "- Build WordCloud.\n",
    "- Build Search Engines.\n",
    "- Build knowledge-graph(aka ontologies).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics are generally important words in text. \n",
    "- Frequency count can be one of the way to identify topics.\n",
    "- TF-IDF can also be used for Topic Modelling.\n",
    "- Or most famous, LDA (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you have the following set of sentences:\n",
    "\n",
    "- I like to eat broccoli and bananas.\n",
    "- I ate a banana and spinach smoothie for breakfast.\n",
    "- Chinchillas and kittens are cute.\n",
    "- My sister adopted a kitten yesterday.\n",
    "- Look at this cute hamster munching on a piece of broccoli.\n",
    "\n",
    "LDA will try to identify words which have been used in similar context and will calculate probability of occuring two words togther.\n",
    "In the above example, LDA will create topics like:\n",
    "    \n",
    "- Topic A: 30% broccoli, 15% bananas, 10% breakfast, 10% munching, … (at which point, you could interpret topic A to be about food)\n",
    "- Topic B: 20% chinchillas, 20% kittens, 20% cute, 15% hamster, … (at which point, you could interpret topic B to be about cute animals).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How LDA Works?\n",
    "\n",
    "LDA involves a detailed understanding of Baysian Probabilistic Approach. However, here is an intuitive explanation of how LDA operates:\n",
    "\n",
    "Take the example given above:\n",
    "\n",
    "* Step 1: We start with assigning the tokenizing and removing stopwords\n",
    "* Step 2: We decide the number of topics, in our case we have decided that number to be 2.\n",
    "* Step 3: Then we take each token in each document and randomly assign it to a topic\n",
    "    * For Statement 1 it might look like this:\n",
    "        * Like: 2\n",
    "        * Eat: 2\n",
    "        * Broccoli: 1\n",
    "        * banana: 2\n",
    "        * Repeat this process for each statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 4: Doing this will result in 2 matrices:\n",
    "    1. Document to Topic probability distribution (S -> T)\n",
    "    2. Topic to token probability distribution (T -> W)\n",
    "        * Since these are distributed randomly, they are not accurate\n",
    "        * Hence, we want to modify both matrices to make them as near to the real distribution as possible\n",
    "* Step 5: We again iterate through each token in each document and again assign the topic considering 2 things\n",
    "    * How prevalent is that word across topics? P(word W| Topic T)\n",
    "    * How prevalent are topics in the document? P(Topic T| Document D)\n",
    "    * Let's consider word \"eat\".\n",
    "    * Since it only appears in Topic 2 in statement 1, Hence it is only associated with topic 2.\n",
    "    * Statement 1 is made up of  3/4 of Topic 2 and  1/4 of Topic 1\n",
    "    * This can be interpreted as: word \"eat\" is highly specific to topic 2 and topic 2 makes up of majority of statement 1.\n",
    "    * Hence, eat is more likely to belong to topic 2\n",
    "    * So, we assign \"eat\" to topic 2\n",
    "* Step 6:\n",
    "    * Go to step 4\n",
    "    * We repeat this procedure for each token\n",
    "    * If we perform this entire procedure again and again, we will attain the (S->T) and (T->W) which are approximately equal to actual matrices.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/4p9MSJy761Y?start=801&end=1043\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/4p9MSJy761Y?start=801&end=1043\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim(https://radimrehurek.com/gensim/) package in python implements most of topic modelling algorithms.\n",
    "\n",
    "* We'll walk through a basic application of Topic Modeling with LDA\n",
    "* We'll also cover the basic NLP operations necessary for the application\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample documents\n",
    "# We will use author data\n",
    "import nltk\n",
    "text=open(\"../data/C50train/AaronPressman/2537newsML.txt\").read()\n",
    "sents = nltk.sent_tokenize(text)\n",
    "\n",
    "# compile documents\n",
    "doc_complete =sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's fast-forward through pre-processing\n",
    "\n",
    "* After the processing, we'll have *texts* - a tokenized, stopped and stemmed list of words from a single document\n",
    "* Let’s fast forward and loop through all our documents and appended each one to *texts*\n",
    "* So now *texts* is a list of lists, one list for each of our original documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from pprint import pprint\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = set(stopwords.words('english'))\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "   \n",
    "# create sample documents\n",
    "# We will use author data\n",
    "import nltk\n",
    "text=open(\"../data/C50train/AaronPressman/2537newsML.txt\").read()\n",
    "sents = nltk.sent_tokenize(text)\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = sents\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### texts\n",
      "[['break', 'u', 'justic', 'depart', 'world', 'wide', 'web', 'site', 'last', 'week', 'highlight', 'internet', 'continu', 'vulner', 'hacker'], ['unidentifi', 'hacker', 'gain', 'access', 'depart', 'web', 'page', 'august', '16', 'replac', 'hate', 'fill', 'diatrib', 'label', 'depart', 'injustic', 'includ', 'swastika', 'pictur', 'adolf', 'hitler'], ['justic', 'offici', 'quickli', 'pull', 'plug', 'vandalis', 'page', 'secur', 'flaw', 'allow', 'hacker', 'gain', 'entri', 'like', 'exist', 'thousand', 'corpor', 'govern', 'web', 'site', 'secur', 'expert', 'said'], ['vast', 'major', 'site', 'vulner', 'said', 'richard', 'power', 'senior', 'analyst', 'comput', 'secur', 'institut'], ['justic', 'depart', 'singl'], ['justic', 'depart', 'offici', 'said', 'compromis', 'web', 'site', 'connect', 'comput', 'contain', 'sensit', 'file'], ['web', 'site', 'http', 'www', 'usdoj', 'gov', 'includ', 'copi', 'press', 'releas', 'speech', 'publicli', 'avail', 'inform'], ['secur', 'breach', 'like', 'graffiti', 'outsid', 'build', 'spokesman', 'bert', 'brandenburg', 'said'], ['organis', 'target', 'past'], ['last', 'year', 'nation', 'islam', 'million', 'man', 'march', 'web', 'site', 'vandalis'], ['hacker', 'make', '250', '000', 'attempt', 'annual', 'break', 'u', 'militari', 'comput', 'accord', 'gener', 'account', 'offic', 'report'], ['window', 'magazin', 'recent', 'found', 'secur', 'flaw', 'web', 'site', 'dozen', 'major', 'corpor'], ['web', 'spectacularli', 'insecur', 'editor', 'mike', 'elgan', 'said'], ['reli', 'secur', 'hole', 'document', 'softwar', 'manufactur', 'month', 'earlier', 'magazin', 'specialist', 'abl', 'gain', 'variou', 'degre', 'unauthoris', 'access', 'differ', 'site'], ['elgan', 'said', 'hacker', 'exploit', 'flaw', 'motiv', 'anger', 'growth', 'commerci', 'internet'], ['common', 'theme', 'hacker', 'fed', 'non', 'hacker', 'internet', 'said'], ['battl', 'complet', 'hopeless'], ['secur', 'web', 'site', 'richard', 'power', 'said'], ['kind', 'measur', 'take'], ['corpor', 'institut', 'take', 'simpli', 'noth', 'bad', 'happen', 'yet'], ['site', 'use', 'multipl', 'layer', 'secur', 'well', 'beyond', 'simpl', 'password', 'protect', 'keep', 'hacker'], ['one', 'site', 'mention', 'window', 'magazin', 'fidel', 'invest'], ['fidel', 'site', 'advertis', 'mutual', 'fund', 'dissemin', 'inform', 'person', 'financ', 'contain', 'confidenti', 'custom', 'inform'], ['fidel', 'offici', 'immedi', 'close', 'loophol', 'identifi', 'magazin', 'spokeswoman', 'said'], ['multipl', 'secur', 'measur', 'previous', 'place', 'would', 'prevent', 'secur', 'breach', 'despit', 'hole', 'spokeswoman', 'ad']]\n",
      "\n",
      "##### The lines in texts\n",
      "['break', 'u', 'justic', 'depart', 'world', 'wide', 'web', 'site', 'last', 'week', 'highlight', 'internet', 'continu', 'vulner', 'hacker']\n",
      "['unidentifi', 'hacker', 'gain', 'access', 'depart', 'web', 'page', 'august', '16', 'replac', 'hate', 'fill', 'diatrib', 'label', 'depart', 'injustic', 'includ', 'swastika', 'pictur', 'adolf', 'hitler']\n",
      "['justic', 'offici', 'quickli', 'pull', 'plug', 'vandalis', 'page', 'secur', 'flaw', 'allow', 'hacker', 'gain', 'entri', 'like', 'exist', 'thousand', 'corpor', 'govern', 'web', 'site', 'secur', 'expert', 'said']\n",
      "['vast', 'major', 'site', 'vulner', 'said', 'richard', 'power', 'senior', 'analyst', 'comput', 'secur', 'institut']\n",
      "['justic', 'depart', 'singl']\n",
      "['justic', 'depart', 'offici', 'said', 'compromis', 'web', 'site', 'connect', 'comput', 'contain', 'sensit', 'file']\n",
      "['web', 'site', 'http', 'www', 'usdoj', 'gov', 'includ', 'copi', 'press', 'releas', 'speech', 'publicli', 'avail', 'inform']\n",
      "['secur', 'breach', 'like', 'graffiti', 'outsid', 'build', 'spokesman', 'bert', 'brandenburg', 'said']\n",
      "['organis', 'target', 'past']\n",
      "['last', 'year', 'nation', 'islam', 'million', 'man', 'march', 'web', 'site', 'vandalis']\n",
      "['hacker', 'make', '250', '000', 'attempt', 'annual', 'break', 'u', 'militari', 'comput', 'accord', 'gener', 'account', 'offic', 'report']\n",
      "['window', 'magazin', 'recent', 'found', 'secur', 'flaw', 'web', 'site', 'dozen', 'major', 'corpor']\n",
      "['web', 'spectacularli', 'insecur', 'editor', 'mike', 'elgan', 'said']\n",
      "['reli', 'secur', 'hole', 'document', 'softwar', 'manufactur', 'month', 'earlier', 'magazin', 'specialist', 'abl', 'gain', 'variou', 'degre', 'unauthoris', 'access', 'differ', 'site']\n",
      "['elgan', 'said', 'hacker', 'exploit', 'flaw', 'motiv', 'anger', 'growth', 'commerci', 'internet']\n",
      "['common', 'theme', 'hacker', 'fed', 'non', 'hacker', 'internet', 'said']\n",
      "['battl', 'complet', 'hopeless']\n",
      "['secur', 'web', 'site', 'richard', 'power', 'said']\n",
      "['kind', 'measur', 'take']\n",
      "['corpor', 'institut', 'take', 'simpli', 'noth', 'bad', 'happen', 'yet']\n",
      "['site', 'use', 'multipl', 'layer', 'secur', 'well', 'beyond', 'simpl', 'password', 'protect', 'keep', 'hacker']\n",
      "['one', 'site', 'mention', 'window', 'magazin', 'fidel', 'invest']\n",
      "['fidel', 'site', 'advertis', 'mutual', 'fund', 'dissemin', 'inform', 'person', 'financ', 'contain', 'confidenti', 'custom', 'inform']\n",
      "['fidel', 'offici', 'immedi', 'close', 'loophol', 'identifi', 'magazin', 'spokeswoman', 'said']\n",
      "['multipl', 'secur', 'measur', 'previous', 'place', 'would', 'prevent', 'secur', 'breach', 'despit', 'hole', 'spokeswoman', 'ad']\n"
     ]
    }
   ],
   "source": [
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "print(\"\\n##### texts\")\n",
    "print(texts)\n",
    "\n",
    "print(\"\\n##### The lines in texts\")\n",
    "for line in texts:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What's next?\n",
    "\n",
    "* To generate an LDA model, we need to understand how frequently each term occurs within each document\n",
    "* To do that, we need to construct a document-term matrix with a package called *gensim*\n",
    "\n",
    "# Topic Modeling with gensim\n",
    "\n",
    "## Getting started with gensim?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(175 unique tokens: ['break', 'continu', 'depart', 'hacker', 'highlight']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The Dictionary() function traverses texts, assigning a unique integer id to each unique token while also collecting word counts and relevant statistics\n",
    "* To see each token’s unique integer id, try -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'break': 0, 'continu': 1, 'depart': 2, 'hacker': 3, 'highlight': 4, 'internet': 5, 'justic': 6, 'last': 7, 'site': 8, 'u': 9, 'vulner': 10, 'web': 11, 'week': 12, 'wide': 13, 'world': 14, '16': 15, 'access': 16, 'adolf': 17, 'august': 18, 'diatrib': 19, 'fill': 20, 'gain': 21, 'hate': 22, 'hitler': 23, 'includ': 24, 'injustic': 25, 'label': 26, 'page': 27, 'pictur': 28, 'replac': 29, 'swastika': 30, 'unidentifi': 31, 'allow': 32, 'corpor': 33, 'entri': 34, 'exist': 35, 'expert': 36, 'flaw': 37, 'govern': 38, 'like': 39, 'offici': 40, 'plug': 41, 'pull': 42, 'quickli': 43, 'said': 44, 'secur': 45, 'thousand': 46, 'vandalis': 47, 'analyst': 48, 'comput': 49, 'institut': 50, 'major': 51, 'power': 52, 'richard': 53, 'senior': 54, 'vast': 55, 'singl': 56, 'compromis': 57, 'connect': 58, 'contain': 59, 'file': 60, 'sensit': 61, 'avail': 62, 'copi': 63, 'gov': 64, 'http': 65, 'inform': 66, 'press': 67, 'publicli': 68, 'releas': 69, 'speech': 70, 'usdoj': 71, 'www': 72, 'bert': 73, 'brandenburg': 74, 'breach': 75, 'build': 76, 'graffiti': 77, 'outsid': 78, 'spokesman': 79, 'organis': 80, 'past': 81, 'target': 82, 'islam': 83, 'man': 84, 'march': 85, 'million': 86, 'nation': 87, 'year': 88, '000': 89, '250': 90, 'accord': 91, 'account': 92, 'annual': 93, 'attempt': 94, 'gener': 95, 'make': 96, 'militari': 97, 'offic': 98, 'report': 99, 'dozen': 100, 'found': 101, 'magazin': 102, 'recent': 103, 'window': 104, 'editor': 105, 'elgan': 106, 'insecur': 107, 'mike': 108, 'spectacularli': 109, 'abl': 110, 'degre': 111, 'differ': 112, 'document': 113, 'earlier': 114, 'hole': 115, 'manufactur': 116, 'month': 117, 'reli': 118, 'softwar': 119, 'specialist': 120, 'unauthoris': 121, 'variou': 122, 'anger': 123, 'commerci': 124, 'exploit': 125, 'growth': 126, 'motiv': 127, 'common': 128, 'fed': 129, 'non': 130, 'theme': 131, 'battl': 132, 'complet': 133, 'hopeless': 134, 'kind': 135, 'measur': 136, 'take': 137, 'bad': 138, 'happen': 139, 'noth': 140, 'simpli': 141, 'yet': 142, 'beyond': 143, 'keep': 144, 'layer': 145, 'multipl': 146, 'password': 147, 'protect': 148, 'simpl': 149, 'use': 150, 'well': 151, 'fidel': 152, 'invest': 153, 'mention': 154, 'one': 155, 'advertis': 156, 'confidenti': 157, 'custom': 158, 'dissemin': 159, 'financ': 160, 'fund': 161, 'mutual': 162, 'person': 163, 'close': 164, 'identifi': 165, 'immedi': 166, 'loophol': 167, 'spokeswoman': 168, 'ad': 169, 'despit': 170, 'place': 171, 'prevent': 172, 'previous': 173, 'would': 174}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next, our dictionary must be converted into a [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1)]\n",
      "[(2, 2), (3, 1), (11, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1)]\n",
      "[(3, 1), (6, 1), (8, 1), (11, 1), (21, 1), (27, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 2), (46, 1), (47, 1)]\n",
      "[(8, 1), (10, 1), (44, 1), (45, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1)]\n",
      "[(2, 1), (6, 1), (56, 1)]\n",
      "[(2, 1), (6, 1), (8, 1), (11, 1), (40, 1), (44, 1), (49, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1)]\n",
      "[(8, 1), (11, 1), (24, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1)]\n",
      "[(39, 1), (44, 1), (45, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1)]\n",
      "[(80, 1), (81, 1), (82, 1)]\n",
      "[(7, 1), (8, 1), (11, 1), (47, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1)]\n",
      "[(0, 1), (3, 1), (9, 1), (49, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1)]\n",
      "[(8, 1), (11, 1), (33, 1), (37, 1), (45, 1), (51, 1), (100, 1), (101, 1), (102, 1), (103, 1), (104, 1)]\n",
      "[(11, 1), (44, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1)]\n",
      "[(8, 1), (16, 1), (21, 1), (45, 1), (102, 1), (110, 1), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1)]\n",
      "[(3, 1), (5, 1), (37, 1), (44, 1), (106, 1), (123, 1), (124, 1), (125, 1), (126, 1), (127, 1)]\n",
      "[(3, 2), (5, 1), (44, 1), (128, 1), (129, 1), (130, 1), (131, 1)]\n",
      "[(132, 1), (133, 1), (134, 1)]\n",
      "[(8, 1), (11, 1), (44, 1), (45, 1), (52, 1), (53, 1)]\n",
      "[(135, 1), (136, 1), (137, 1)]\n",
      "[(33, 1), (50, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1)]\n",
      "[(3, 1), (8, 1), (45, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 1), (148, 1), (149, 1), (150, 1), (151, 1)]\n",
      "[(8, 1), (102, 1), (104, 1), (152, 1), (153, 1), (154, 1), (155, 1)]\n",
      "[(8, 1), (59, 1), (66, 2), (152, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1)]\n",
      "[(40, 1), (44, 1), (102, 1), (152, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 1)]\n",
      "[(45, 2), (75, 1), (115, 1), (136, 1), (146, 1), (168, 1), (169, 1), (170, 1), (171, 1), (172, 1), (173, 1), (174, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "for line in corpus:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The doc2bow() function converts dictionary into a bag-of-words\n",
    "* The result, *corpus*, is a list of vectors equal to the number of documents\n",
    "* In each document vector is a series of tuples\n",
    "* The tuples are (term ID, term frequency) pairs\n",
    "* This includes terms that actually occur - terms that do not occur in a document will not appear in that document’s vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating the LDA Model\n",
    "\n",
    "*corpus* is a (sparse) document-term matrix and now we’re ready to generate an LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parameters to the LDA model\n",
    "\n",
    "https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "* num_topics\n",
    "    - required\n",
    "    - An LDA model requires the user to determine how many topics should be generated\n",
    "    - Our document set is small, so we’re only asking for three topics\n",
    "* id2word\n",
    "    - required\n",
    "    - The LdaModel class requires our previous dictionary to map ids to strings\n",
    "* passes\n",
    "    - optional\n",
    "    - The number of laps the model will take through corpus\n",
    "    - The greater the number of passes, the more accurate the model will be\n",
    "    - A lot of passes can be slow on a very large corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel(num_terms=175, num_topics=3, decay=0.5, chunksize=2000)\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.042*\"secur\" + 0.036*\"site\" + 0.030*\"web\" + 0.025*\"depart\" + 0.019*\"justic\" + 0.019*\"hacker\" + 0.019*\"gain\" + 0.019*\"said\" + 0.013*\"offici\" + 0.013*\"multipl\"'), (1, '0.023*\"magazin\" + 0.023*\"fidel\" + 0.023*\"said\" + 0.023*\"hacker\" + 0.023*\"site\" + 0.013*\"window\" + 0.013*\"spokeswoman\" + 0.013*\"includ\" + 0.013*\"inform\" + 0.013*\"avail\"'), (2, '0.027*\"said\" + 0.027*\"site\" + 0.021*\"hacker\" + 0.020*\"web\" + 0.020*\"secur\" + 0.014*\"comput\" + 0.014*\"internet\" + 0.014*\"flaw\" + 0.014*\"corpor\" + 0.014*\"major\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.042*\"secur\" + 0.036*\"site\" + 0.030*\"web\" + 0.025*\"depart\" + 0.019*\"justic\" + 0.019*\"hacker\" + 0.019*\"gain\" + 0.019*\"said\" + 0.013*\"offici\" + 0.013*\"multipl\"')\n",
      "(2, '0.027*\"said\" + 0.027*\"site\" + 0.021*\"hacker\" + 0.020*\"web\" + 0.020*\"secur\" + 0.014*\"comput\" + 0.014*\"internet\" + 0.014*\"flaw\" + 0.014*\"corpor\" + 0.014*\"major\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in ldamodel.print_topics(num_topics=2):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.042*\"secur\" + 0.036*\"site\" + 0.030*\"web\"')\n",
      "(1, '0.023*\"magazin\" + 0.023*\"fidel\" + 0.023*\"said\"')\n",
      "(2, '0.027*\"said\" + 0.027*\"site\" + 0.021*\"hacker\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in ldamodel.print_topics(num_topics=3, num_words=3):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Within each topic are the three most probable words to appear in that topic\n",
    "\n",
    "## Topics in detail\n",
    "Let's now look at a topic in detail. Let us see how distinct the topics are, and if they seem to capture any context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.042*\"secur\" + 0.036*\"site\" + 0.030*\"web\" + 0.025*\"depart\" + 0.019*\"justic\" + 0.019*\"hacker\" + 0.019*\"gain\" + 0.019*\"said\" + 0.013*\"offici\" + 0.013*\"multipl\"\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topic(topicno=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023*\"magazin\" + 0.023*\"fidel\" + 0.023*\"said\" + 0.023*\"hacker\" + 0.023*\"site\" + 0.013*\"window\" + 0.013*\"spokeswoman\" + 0.013*\"includ\" + 0.013*\"inform\" + 0.013*\"avail\"\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topic(topicno=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.027*\"said\" + 0.027*\"site\" + 0.021*\"hacker\" + 0.020*\"web\" + 0.020*\"secur\" + 0.014*\"comput\" + 0.014*\"internet\" + 0.014*\"flaw\" + 0.014*\"corpor\" + 0.014*\"major\"\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topic(topicno=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Do the topics make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.042*\"secur\" + 0.036*\"site\" + 0.030*\"web\"')\n",
      "(1, '0.023*\"magazin\" + 0.023*\"fidel\" + 0.023*\"said\"')\n",
      "(2, '0.027*\"said\" + 0.027*\"site\" + 0.021*\"hacker\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in ldamodel.print_topics(num_topics=3, num_words=3):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Refining the model\n",
    "\n",
    "Two topics seems like a better fit for our documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.027*\"web\" + 0.023*\"depart\" + 0.021*\"site\" + 0.017*\"hacker\"')\n",
      "(1, '0.042*\"secur\" + 0.036*\"site\" + 0.028*\"said\" + 0.023*\"hacker\"')\n"
     ]
    }
   ],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)\n",
    "\n",
    "for topic in ldamodel.print_topics(num_topics=2, num_words=4):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try it with more passes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.030*\"said\" + 0.025*\"web\" + 0.025*\"hacker\" + 0.021*\"depart\"')\n",
      "(1, '0.039*\"site\" + 0.030*\"secur\" + 0.020*\"web\" + 0.016*\"inform\"')\n"
     ]
    }
   ],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=200)\n",
    "\n",
    "for topic in ldamodel.print_topics(num_topics=2, num_words=4):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predicting Topic for new documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "doc_f = \"Are Health professionals justified in saying that brocolli is good for your health?\" \n",
    "\n",
    "doc_set = [doc_f]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "infer = ldamodel[corpus[0]]\n",
    "\n",
    "# https://radimrehurek.com/gensim/wiki.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel(num_terms=175, num_topics=2, decay=0.5, chunksize=2000)\n"
     ]
    }
   ],
   "source": [
    "#Lets check by default LDA parameters\n",
    "print(ldamodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Tree Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One of the advance topic in NLP is Lexical Analysis of text wherein we try to analyze and understand text. This process is called deep tree parsing in NLP world where we try to analyze relationships amongst the text.\n",
    "- Text parsing is important when you want to know relationships in text. For example <i>Delhi is capital of India<i>, here Delhi and India are related and having a relationship <b>is capital of<b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (Det the) (N dog))\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (Det a) (N man) (PP (P in) (NP (Det the) (N park))))))\n",
      "(S\n",
      "  (NP (Det the) (N dog))\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (Det a) (N man))\n",
      "    (PP (P in) (NP (Det the) (N park)))))\n"
     ]
    }
   ],
   "source": [
    "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  VP -> V NP | V NP PP\n",
    "  PP -> P NP\n",
    "  V -> \"saw\" | \"ate\" | \"walked\"\n",
    "  NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n",
    "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
    "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
    "  P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
    "  \"\"\")\n",
    "sent = \"the dog saw a man in the park\"\n",
    "tokens=nltk.word_tokenize(sent)\n",
    "rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
    "for tree in rd_parser.parse(tokens):\n",
    "    print(tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![alt text](../images/deep_parsing.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here as well we have to define our grammar, which looks quite tedious job. But there are other NLP packages such as Stanford CoreNLP which provides funcitons to generate parse tree from unstructured text without defining any grammar.\n",
    "- Parse tree provides us meaningful and true relations and also kind of relations they share. Also called facts.\n",
    "- Tree Parsing is used to build knowledge base from unstructured corpus. Check DbPedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-session Recap Time\n",
    "***\n",
    "- POS Tagging\n",
    "- Chunking\n",
    "- Topic Modelling\n",
    "- Latent Dirichlet Allocation\n",
    "- Gensim\n",
    "- Deep Tree Parsing\n",
    "\n",
    "# Thank You\n",
    "\n",
    "For more queries - Reach out to academics@greyatom.com "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
